import os
import sys
import json
from dotenv import load_dotenv
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from sentence_transformers import SentenceTransformer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import settings
from app.core.logger import logger

# é…ç½®
MILVUS_HOST = settings.MILVUS_HOST
MILVUS_PORT = settings.MILVUS_PORT
COLLECTION_NAME = settings.MILVUS_COLLECTION

# è¾“å…¥æ–‡ä»¶ (ETL äº§ç‰©)
SOURCE_FILE = settings.OUT_PATH  # e.g., data/schema_catalog.jsonl

# æ¨¡å‹é…ç½®
EMBED_MODEL = settings.EMBED_MODEL
BATCH_SIZE = 64
TEXT_MAX_LEN = 8192  # å…è®¸æ›´é•¿çš„ Rich Text


def init_milvus(dim: int) -> Collection:
    logger.info(f"ğŸ”Œ Connecting to Milvus {MILVUS_HOST}:{MILVUS_PORT}...")
    connections.connect(alias="default", host=MILVUS_HOST, port=MILVUS_PORT)

    # ğŸ”¥ æ¯æ¬¡å…¨é‡æ›´æ–°æ—¶ï¼Œå…ˆåˆ é™¤æ—§é›†åˆï¼Œé˜²æ­¢ Schema å†²çª
    if utility.has_collection(COLLECTION_NAME):
        logger.warning(f"ğŸ—‘ï¸ Dropping existing collection: {COLLECTION_NAME}")
        utility.drop_collection(COLLECTION_NAME)

    logger.info(f"ğŸ”¨ Creating collection: {COLLECTION_NAME}")

    fields = [
        # 1. ä¸»é”® (Primary Key)
        FieldSchema(name="full_name", dtype=DataType.VARCHAR, max_length=256, is_primary=True),

        # 2. å‘é‡ (Embedding) - æ ¸å¿ƒæ£€ç´¢ä¾æ®
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dim),

        # 3. åŸºç¡€å®šä½ä¿¡æ¯
        FieldSchema(name="db", dtype=DataType.VARCHAR, max_length=128),
        FieldSchema(name="logical_table", dtype=DataType.VARCHAR, max_length=128),

        # 4. æ²»ç†å…ƒæ•°æ® (ç”¨äºè¿‡æ»¤/Gate)
        FieldSchema(name="domain", dtype=DataType.VARCHAR, max_length=64),
        FieldSchema(name="risk_level", dtype=DataType.VARCHAR, max_length=32),
        FieldSchema(name="table_type", dtype=DataType.VARCHAR, max_length=32),

        # 5. æ ¸å¿ƒè¯­ä¹‰æ–‡æœ¬ (Rich Text: é”šç‚¹+æ€»ç»“+Schema+æ ·æœ¬)
        # æ³¨æ„ï¼šMilvus VARCHAR æœ€å¤§æ”¯æŒ 65535ï¼Œè¿™é‡Œè®¾ 8192 è¶³å¤Ÿäº†
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=TEXT_MAX_LEN),
    ]

    schema = CollectionSchema(fields, description="TableCard V2: Semantic Catalog")
    col = Collection(COLLECTION_NAME, schema)

    # åˆ›å»ºç´¢å¼• (HNSW æ€§èƒ½æœ€å¥½)
    index_params = {
        "index_type": "HNSW",
        "metric_type": "IP",  # Inner Product (é…åˆå½’ä¸€åŒ– Embedding ç­‰ä»·äº Cosine)
        "params": {"M": 16, "efConstruction": 200},
    }
    col.create_index(field_name="embedding", index_params=index_params)
    logger.info("âœ… Collection & Index created.")
    return col


def insert_batch(col: Collection, model: SentenceTransformer, batch: list[dict]):
    # æå–ç”¨äº Embedding çš„æ–‡æœ¬
    texts = [x["raw_text_for_emb"] for x in batch]

    # è®¡ç®—å‘é‡ (Normalize=True å¾ˆé‡è¦ï¼Œä¾¿äºåç»­ç”¨ IP ç®—åˆ†)
    embeddings = model.encode(texts, normalize_embeddings=True)

    # æ’å…¥æ•°æ® (æ³¨æ„é¡ºåºå¿…é¡»å’Œ Schema definition ä¸€è‡´)
    data = [
        [x["full_name"] for x in batch],  # full_name
        embeddings.tolist(),  # embedding
        [x["db"] for x in batch],  # db
        [x["logical_table"] for x in batch],  # logical_table
        [x["domain"] for x in batch],  # domain
        [x["risk_level"] for x in batch],  # risk_level
        [x["table_type"] for x in batch],  # table_type
        [x["text"] for x in batch],  # text
    ]

    col.insert(data)


def main():
    if not os.path.exists(SOURCE_FILE):
        logger.error(f"âŒ File not found: {SOURCE_FILE}. Please run extract_schema_to_jsonl.py first.")
        return

    logger.info(f"ğŸ§  Loading embedding model: {EMBED_MODEL}...")
    try:
        model = SentenceTransformer(EMBED_MODEL)
    except Exception as e:
        logger.error(f"âŒ Model load failed: {e}")
        return

    # æµ‹ç®—ç»´åº¦
    test_emb = model.encode(["test"], normalize_embeddings=True)
    dim = int(test_emb.shape[1])
    logger.info(f"ğŸ“ Vector dimension: {dim}")

    # åˆå§‹åŒ– Milvus
    col = init_milvus(dim)

    inserted = 0
    batch = []

    logger.info(f"ğŸš€ Processing data from {SOURCE_FILE}...")
    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue

            try:
                card = json.loads(line)
            except json.JSONDecodeError:
                continue

            ident = card.get("identity", {})
            llm_info = card.get("llm", {})

            # è¿™é‡Œçš„ text å·²ç»æ˜¯ ETL ç”Ÿæˆå¥½çš„ Rich Text (å¸¦é”šç‚¹å’Œæ¸…æ´—è¿‡çš„åŒä¹‰è¯)
            raw_text = card.get("text", "")
            safe_text = raw_text[:TEXT_MAX_LEN]

            # æ„é€  full_name (ä¸»é”®)
            # æ³¨æ„ï¼šè¿™é‡Œçš„ logical_table å·²ç»æ˜¯å½’ä¸€åŒ–åçš„ (å¦‚ t_order)
            full_name = f"{ident.get('db')}.{ident.get('logical_table')}"

            entry = {
                "full_name": full_name,
                "db": ident.get("db", ""),
                "logical_table": ident.get("logical_table", ""),
                "domain": ident.get("domain", "unknown"),

                "risk_level": llm_info.get("risk_level", "normal"),
                "table_type": llm_info.get("table_type", "unknown"),

                "text": safe_text,
                "raw_text_for_emb": raw_text
            }
            batch.append(entry)

            if len(batch) >= BATCH_SIZE:
                insert_batch(col, model, batch)
                inserted += len(batch)
                print(f"  âœ… Inserted: {inserted}")
                batch = []

    if batch:
        insert_batch(col, model, batch)
        inserted += len(batch)
        print(f"  âœ… Inserted: {inserted}")

    # åˆ·ç›˜å¹¶åŠ è½½åˆ°å†…å­˜ï¼Œå‡†å¤‡æŸ¥è¯¢
    col.flush()
    # col.load() # æš‚æ—¶ä¸ Loadï¼Œç•™ç»™ retrieve_tables.py æ‡’åŠ è½½

    logger.info(f"ğŸ‰ All Done! Total {col.num_entities} entities indexed in '{COLLECTION_NAME}'.")


if __name__ == "__main__":
    main()