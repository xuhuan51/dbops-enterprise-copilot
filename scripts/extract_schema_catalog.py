import os
import re
import sys
import json
import pymysql
import datetime
import concurrent.futures
from decimal import Decimal
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.core.config import settings
from app.core.llm import chat_completion
from app.core.prompts import TABLE_CARD_GOVERNANCE_PROMPT
from app.core.logger import logger

OUTPUT_FILE = settings.OUT_PATH
MAX_WORKERS = 5  # ğŸ”¥ å¹¶å‘æ•° (æ ¹æ®ä½ çš„ LLM Rate Limit è°ƒæ•´ï¼Œå¤ªé«˜ä¼šæŠ¥é”™)

# ==========================================
# ğŸ§¹ æ ¸å¿ƒæ¸…æ´—é€»è¾‘ (Quality Control)
# ==========================================
SYNONYM_BLACKLIST = re.compile(r"(è¡¨|è®°å½•|æ•°æ®|ä¿¡æ¯|ç®¡ç†|æœåŠ¡|åˆ—è¡¨|æ˜ç»†)$")


def clean_synonyms(synonyms: list, table_name: str) -> list:
    """
    æ¸…æ´—åŒä¹‰è¯ï¼š
    1. å»æ‰åŒ…å« 'è¡¨', 'è®°å½•' ç­‰æ³›è¯çš„è¯
    2. å»æ‰å’Œè¡¨åå®Œå…¨ä¸€æ ·çš„è¯
    3. é™åˆ¶æ•°é‡ (Top 5)
    """
    clean = []
    seen = set()

    # ä¼˜å…ˆä¿ç•™çŸ­è¯ (é€šå¸¸æ˜¯æ ¸å¿ƒæ¦‚å¿µ)
    for w in sorted(synonyms, key=len):
        w = w.strip()
        # è¿‡æ»¤ç©ºã€è¿‡æ»¤è¡¨åæœ¬èº«ã€è¿‡æ»¤æ³›è¯åç¼€
        if not w or w == table_name:
            continue
        if len(w) > 10:  # å¤ªé•¿çš„è¯é€šå¸¸æ˜¯è§£é‡Šï¼Œä¸æ˜¯åŒä¹‰è¯
            continue
        if SYNONYM_BLACKLIST.search(w):
            continue

        if w not in seen:
            clean.append(w)
            seen.add(w)

    return clean[:5]


def extract_key_fields(columns_desc: str) -> str:
    """
    ä» Schema æè¿°ä¸­æå–ç¡¬é”šç‚¹ (Key Fields)
    è§„åˆ™ï¼šæå–ä¸»é”®ã€å¤–é”®(_id)ã€æ—¶é—´(_time/_date)ã€çŠ¶æ€(status/type)
    """
    keys = []
    lines = columns_desc.split('\n')
    for line in lines:
        # line æ ¼å¼: "- order_id (bigint) [PK]: è®¢å•ID"
        # ç®€å•æ­£åˆ™æå–å­—æ®µå
        match = re.search(r"- (\w+)", line)
        if not match: continue
        col_name = match.group(1).lower()

        # é”šç‚¹ç­–ç•¥
        if " [PK]" in line:  # ä¸»é”®å¿…é€‰
            keys.append(col_name)
        elif col_name.endswith("_id") or col_name.endswith("_code"):  # å¤–é”®/ç¼–ç 
            keys.append(col_name)
        elif "status" in col_name or "type" in col_name:  # æ ¸å¿ƒç»´åº¦
            keys.append(col_name)
        elif "amount" in col_name or "price" in col_name or "gmv" in col_name:  # æ ¸å¿ƒæŒ‡æ ‡
            keys.append(col_name)

    # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢ Token çˆ†ç‚¸
    return ", ".join(keys[:8])


# ==========================================
# åŸºç¡€å·¥å…·
# ==========================================
def get_connection():
    # ğŸ”¥ æ³¨æ„ï¼šåœ¨å¤šçº¿ç¨‹é‡Œï¼Œæ¯ä¸ªçº¿ç¨‹å¿…é¡»åˆ›å»ºè‡ªå·±çš„è¿æ¥ï¼Œä¸èƒ½å…±äº«
    return pymysql.connect(
        host=settings.MYSQL_HOST,
        port=settings.MYSQL_PORT,
        user=settings.MYSQL_USER,
        password=settings.MYSQL_PASSWORD,
        database=settings.MYSQL_CONNECT_DB,
        charset="utf8mb4",
        cursorclass=pymysql.cursors.DictCursor
    )


class DateEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (datetime.datetime, datetime.date)):
            return obj.strftime('%Y-%m-%d %H:%M:%S')
        if isinstance(obj, Decimal):
            return float(obj)
        return super().default(obj)


def get_logical_name(table_name: str) -> str:
    name = re.sub(r'_\d{4}W\d{2,3}$', '', table_name, flags=re.IGNORECASE)
    name = re.sub(r'_\d{8}$', '', name)
    name = re.sub(r'_\d+$', '', name)
    return name


def get_all_tables_list(conn, db_name):
    """åªè´Ÿè´£è·å–è¡¨ååˆ—è¡¨ï¼Œä¸è´Ÿè´£é‡çš„æ•°æ®æ“ä½œ"""
    with conn.cursor() as cur:
        sql = "SELECT table_name, table_comment FROM information_schema.tables WHERE table_schema=%s AND table_type='BASE TABLE' ORDER BY table_name"
        cur.execute(sql, (db_name,))
        rows = cur.fetchall()
        return [{k.lower(): v for k, v in r.items()} for r in rows]


def get_schema_info_str(conn, db_name, table_name):
    with conn.cursor() as cur:
        sql = """
              SELECT column_name, column_type, column_comment, column_key
              FROM information_schema.columns
              WHERE table_schema = %s \
                AND table_name = %s
              ORDER BY ordinal_position \
              """
        cur.execute(sql, (db_name, table_name))
        rows = cur.fetchall()
        columns = [{k.lower(): v for k, v in r.items()} for r in rows]

        col_desc_list = []
        for c in columns:
            comment = c.get('column_comment') or ""
            key = " [PK]" if c.get('column_key') == 'PRI' else ""
            col_desc_list.append(f"- {c['column_name']} ({c['column_type']}){key}: {comment}")

        return "\n".join(col_desc_list)


def get_samples_json(conn, db_name, table_name, limit=3):
    with conn.cursor() as cur:
        try:
            cur.execute(f"SELECT * FROM `{db_name}`.`{table_name}` LIMIT %s", (limit,))
            rows = cur.fetchall()
            if rows:
                rows = [{k.lower(): v for k, v in r.items()} for r in rows]
            return json.dumps(rows, cls=DateEncoder, ensure_ascii=False, indent=None)
        except Exception:
            return "[]"


# ==========================================
# ğŸ§µ çº¿ç¨‹å·¥ä½œå‡½æ•° (Worker)
# ==========================================
def process_single_logical_table(db, logical_name, physical_table, table_comment):
    """
    å•ä¸ªé€»è¾‘è¡¨çš„ ETL å¤„ç†å‡½æ•° (åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ)
    """
    # 1. æ¯ä¸ªçº¿ç¨‹å»ºç«‹ç‹¬ç«‹è¿æ¥
    conn = get_connection()
    try:
        # è·å–å…ƒæ•°æ®
        columns_desc = get_schema_info_str(conn, db, physical_table)
        samples_json = get_samples_json(conn, db, physical_table, limit=3)

        # 2. æå–ç¡¬é”šç‚¹ (Hard Anchors)
        key_fields = extract_key_fields(columns_desc)

        # 3. è°ƒç”¨ LLM (è€—æ—¶æ“ä½œ)
        prompt = TABLE_CARD_GOVERNANCE_PROMPT.format(
            db=db,
            logical_table=logical_name,
            table=physical_table,
            domain="unknown",
            table_comment=table_comment,
            columns_desc=columns_desc,
            samples=samples_json
        )

        try:
            llm_resp = chat_completion(prompt)
            llm_data = json.loads(llm_resp)
        except Exception as e:
            logger.warning(f"âš ï¸ LLM Failed for {logical_name}: {e}")
            llm_data = {"summary": f"{logical_name} æ•°æ®è¡¨", "synonyms": [], "risk_level": "normal",
                        "table_type": "fact"}

        # 4. ğŸ”¥ è´¨é‡ä¼˜åŒ–ï¼šåŒä¹‰è¯æ¸…æ´—
        raw_synonyms = llm_data.get('synonyms', [])
        cleaned_synonyms = clean_synonyms(raw_synonyms, logical_name)

        # 5. ğŸ”¥ è´¨é‡ä¼˜åŒ–ï¼šRich Text ç»“æ„é‡ç»„
        # ä¼˜å…ˆå±•ç¤ºï¼šä¸šåŠ¡åŸŸ -> ç±»å‹ -> å…³é”®å­—æ®µ -> æ€»ç»“ -> åŒä¹‰è¯ -> ç»“æ„
        rich_text = (
            f"è¡¨å: {logical_name}\n"
            f"ä¸šåŠ¡åŸŸ: {llm_data.get('domain_suggestion', 'unknown')}\n"
            f"ç±»å‹: {llm_data.get('table_type', 'fact')}\n"
            f"å…³é”®å­—æ®µ: {key_fields}\n"  # âš“ï¸ ç¡¬é”šç‚¹
            f"ä¸šåŠ¡æè¿°: {llm_data.get('summary', '')}\n"
            f"åŒä¹‰è¯: {','.join(cleaned_synonyms)}\n"  # ğŸ§¹ æ¸…æ´—åçš„
            f"å­—æ®µç»“æ„:\n{columns_desc}\n"
            f"æ ·æœ¬æ•°æ®:\n{samples_json}"
        )

        card = {
            "identity": {
                "db": db,
                "logical_table": logical_name,
                "physical_table_example": physical_table,
                "domain": llm_data.get("domain_suggestion", "unknown")
            },
            "llm": {
                "risk_level": llm_data.get("risk_level", "normal"),
                "table_type": llm_data.get("table_type", "unknown"),
                "summary": llm_data.get("summary", ""),
                "synonyms": cleaned_synonyms  # å­˜æ¸…æ´—åçš„
            },
            "text": rich_text,
            "last_update": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        return card

    except Exception as e:
        logger.error(f"âŒ Error processing {logical_name}: {e}")
        return None
    finally:
        conn.close()


def main():
    logger.info(f"ğŸš€ Start ETL (Concurrency: {MAX_WORKERS})")

    # è·å–ä¸»åº“è¡¨æ¸…å• (è¿™ä¸€æ­¥å¾ˆå¿«ï¼Œå•çº¿ç¨‹å³å¯)
    conn = get_connection()
    target_dbs = settings.TARGET_DBS

    tasks = []  # (db, logical_name, physical_name, comment)

    for db in target_dbs:
        db = db.strip()
        if not db: continue

        logger.info(f"ğŸ“‚ Scanning DB: {db}")
        tables = get_all_tables_list(conn, db)

        # åˆ†è¡¨å½’ä¸€åŒ–
        seen_logical = set()
        for t in tables:
            p_name = t['table_name']
            l_name = get_logical_name(p_name)
            if l_name in seen_logical: continue

            seen_logical.add(l_name)
            # æ·»åŠ åˆ°ä»»åŠ¡åˆ—è¡¨
            tasks.append((db, l_name, p_name, t.get('table_comment', '')))

    conn.close()

    total_tasks = len(tasks)
    logger.info(f"ğŸ“‹ Total Logical Tables to Process: {total_tasks}")

    # çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    results = []

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æäº¤ä»»åŠ¡
        future_to_table = {
            executor.submit(process_single_logical_table, db, l_name, p_name, comment): l_name
            for (db, l_name, p_name, comment) in tasks
        }

        for future in tqdm(concurrent.futures.as_completed(future_to_table), total=total_tasks,
                           desc="Processing Tables"):
            try:
                card = future.result()
                if card:
                    results.append(card)
            except Exception as e:
                logger.error(f"Thread Error: {e}")

    # å†™å…¥æ–‡ä»¶
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for card in results:
            f.write(json.dumps(card, ensure_ascii=False) + "\n")

    logger.info(f"ğŸ‰ ETL Done! Saved {len(results)} tables to {OUTPUT_FILE}")


if __name__ == "__main__":
    main()